{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27bb28ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "import pickle\n",
    "import math\n",
    "import contractions\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e397e727",
   "metadata": {},
   "source": [
    "**Load model and vectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7d5d9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\envs\\neural\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator MultinomialNB from version 1.2.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\anaconda\\envs\\neural\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator BernoulliNB from version 1.2.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\anaconda\\envs\\neural\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator ComplementNB from version 1.2.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\anaconda\\envs\\neural\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.2.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\anaconda\\envs\\neural\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator VotingClassifier from version 1.2.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\anaconda\\envs\\neural\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.2.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\anaconda\\envs\\neural\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.2.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with open('best-model.pickle', 'rb') as f:\n",
    "   my_model =  pickle.load(f)\n",
    "    \n",
    "with open('tfidf_vectorizer.pickle', 'rb') as f:\n",
    "    tfidf_vectorizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9961781",
   "metadata": {},
   "source": [
    "**Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50f0b026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get english stop words (most frequent)\n",
    "stop_words = stopwords.words('english')\n",
    "# Get WordNetLemmatizer to get the context of words\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d7fb9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand contractions for better Text interpretations and stop words removal\n",
    "def expand_contractions(text):\n",
    "    # creating an empty list\n",
    "    expanded_words1 = []\n",
    "    for word in text.split():\n",
    "      # using contractions.fix to expand the shortened words\n",
    "      expanded_words1.append(contractions.fix(word))\n",
    "\n",
    "    text = ' '.join(expanded_words1)\n",
    "\n",
    "    contractions_dict = {\n",
    "        \"'m\": \" am\",\n",
    "        \"'s\": \" is\",\n",
    "        \"'re\": \" are\",\n",
    "        \"'ve\": \" have\",\n",
    "        \"'ll\": \" will\",\n",
    "        \"'d\": \" would\",\n",
    "    }\n",
    "\n",
    "    # Case-insensitive flag for the regular expression\n",
    "    pattern = re.compile(r\"\\b(\" + \"|\".join(re.escape(key) for key in contractions_dict.keys()) + r\")\\b\", re.IGNORECASE)\n",
    "\n",
    "    # Replace contractions with their expanded forms\n",
    "    expanded_text = pattern.sub(lambda match: contractions_dict[match.group(0).lower()], text)\n",
    "\n",
    "    return expanded_text\n",
    "\n",
    "# Remove stop words to reduce the dimensionality size and improve total performance\n",
    "def stopWordsRemoval(sentence):\n",
    "    filtered_sentence = []\n",
    "    # Tokenize the sentence\n",
    "    sentence =  word_tokenize(sentence)\n",
    "    for w in sentence:\n",
    "        if w not in stop_words:\n",
    "            # Only append non stop words\n",
    "            filtered_sentence.append(w)\n",
    "\n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "# This function will be used to help in lemmatization to get pos tag\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "# For lemmatization (Lemmatization is similar to stemming but it brings context to the words)\n",
    "def lemmatization(sentence):\n",
    "    filtered_sentence = []\n",
    "    sentence = word_tokenize(sentence)\n",
    "    sentence = pos_tag(sentence)\n",
    "    tokenCount = len(sentence)\n",
    "    for i in range (0, tokenCount):\n",
    "        t = sentence[i][0] # The token\n",
    "        p = sentence[i][1] # The pos tag\n",
    "        p = get_wordnet_pos(p)\n",
    "        l = lemmatizer.lemmatize(t, pos = p)\n",
    "        filtered_sentence.append(l)\n",
    "\n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    # Matches Twitter handles.\n",
    "    text = re.sub(\"(@[A-Za-z0-9]+)\", \" \",text)\n",
    "\n",
    "    # Matches URLs.\n",
    "    text = re.sub(\"(\\w+:\\/\\/\\S+)\", \" \",text)\n",
    "\n",
    "    # Matches Hashtags\n",
    "    text = re.sub(r'#\\w*', ' ' , text)\n",
    "\n",
    "    # Expand contradictions\n",
    "    text = expand_contractions(text)\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove stop words like are, is, has ...\n",
    "    text = stopWordsRemoval(text)\n",
    "\n",
    "    # Lemmatization brings context to the words\n",
    "    text = lemmatization(text)\n",
    "\n",
    "    # Matches special characters letters, spaces, and tabs.\n",
    "    text = re.sub(\"([^A-Za-z \\t])\", \" \",text)\n",
    "\n",
    "    # Remove extra white spaces\n",
    "    text = \" \".join(text.split())\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "912124c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\n",
    "    0:'Negative',\n",
    "    1:'Positive'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcbc00a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    # Clean text\n",
    "    text = [text_preprocessing(text), ]\n",
    "    \n",
    "    x = tfidf_vectorizer.transform(text)\n",
    "    \n",
    "    y = my_model.predict(x)\n",
    "    \n",
    "    y = y[0]\n",
    "    \n",
    "    return classes[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba1202ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment('This movie is so bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee415867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.1.7:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [10/Dec/2023 00:42:59] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [10/Dec/2023 00:42:59] \"GET /static/css/style.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [10/Dec/2023 00:42:59] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [10/Dec/2023 00:43:06] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [10/Dec/2023 00:43:06] \"GET /static/css/style.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [10/Dec/2023 00:43:09] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [10/Dec/2023 00:43:09] \"GET /static/css/style.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [10/Dec/2023 00:43:12] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [10/Dec/2023 00:43:12] \"GET /static/css/style.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [10/Dec/2023 00:43:21] \"POST /predict HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [10/Dec/2023 00:43:21] \"GET /static/css/style.css HTTP/1.1\" 304 -\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    # Take the text\n",
    "    text = request.form['text']\n",
    "    prediction = predict_sentiment(text)\n",
    "    return render_template('index.html',prediction_text=f\"Sentiment: {prediction}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host=\"0.0.0.0\",port=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7223b92d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
